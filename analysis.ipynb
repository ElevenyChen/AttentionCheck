{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4OUoqVg-zTW"
      },
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Load the dataset\n",
        "# Note: Replace 'your_data_file.csv' with the actual path to your Qualtrics data\n",
        "# df = pd.read_csv('your_data_file.csv')\n",
        "\n",
        "# For demonstration purposes, we'll create a sample dataset structure\n",
        "# In practice, you would load your actual data here\n",
        "print(\"请确保已加载您的数据集到变量 'df' 中\")\n",
        "print(\"数据集应包含以下关键变量：\")\n",
        "print(\"- order: 排列顺序变量\")\n",
        "print(\"- pref_*: 政策偏好问题\")\n",
        "print(\"- base_*: 基线问题\") \n",
        "print(\"- err_*: 错误阈值问题\")\n",
        "print(\"- 各种时间变量: *_First Click, *_Last Click, *_Page Submit, *_Click Count\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Data Preparation and Variable Organization\n",
        "def prepare_data(df):\n",
        "    \"\"\"\n",
        "    Prepare and organize the survey data for analysis\n",
        "    \"\"\"\n",
        "    print(\"开始数据预处理...\")\n",
        "    \n",
        "    # Create a copy to avoid modifying original data\n",
        "    data = df.copy()\n",
        "    \n",
        "    # Define question groups\n",
        "    policy_questions = [col for col in data.columns if col.startswith('pref_') and not any(x in col for x in ['First Click', 'Last Click', 'Page Submit', 'Click Count'])]\n",
        "    baseline_questions = [col for col in data.columns if col.startswith('base_') and not any(x in col for x in ['First Click', 'Last Click', 'Page Submit', 'Click Count'])]\n",
        "    error_questions = [col for col in data.columns if col.startswith('err_') and not any(x in col for x in ['First Click', 'Last Click', 'Page Submit', 'Click Count'])]\n",
        "    \n",
        "    # Define time variables for each group\n",
        "    policy_time_vars = ['pref_First Click', 'pref_Last Click', 'pref_Page Submit', 'pref_Click Count']\n",
        "    \n",
        "    # Baseline time variables (7 themes)\n",
        "    baseline_themes = ['disease', 'armed', 'conv', 'welfare', 'immi', 'vote', 'air', 'firearm', 'auto']\n",
        "    baseline_time_vars = []\n",
        "    for theme in baseline_themes:\n",
        "        baseline_time_vars.extend([f'base_{theme}_First Click', f'base_{theme}_Last Click', \n",
        "                                 f'base_{theme}_Page Submit', f'base_{theme}_Click Count'])\n",
        "    \n",
        "    # Error time variables (7 themes)\n",
        "    error_themes = ['disease', 'armed', 'conv', 'welfare', 'immi', 'vote', 'air', 'auto']\n",
        "    error_time_vars = []\n",
        "    for theme in error_themes:\n",
        "        error_time_vars.extend([f'err_{theme}_First Click', f'err_{theme}_Last Click', \n",
        "                               f'err_{theme}_Page Submit', f'err_{theme}_Click Count'])\n",
        "    \n",
        "    # Store organized data\n",
        "    organized_data = {\n",
        "        'policy_questions': policy_questions,\n",
        "        'baseline_questions': baseline_questions, \n",
        "        'error_questions': error_questions,\n",
        "        'policy_time_vars': policy_time_vars,\n",
        "        'baseline_time_vars': baseline_time_vars,\n",
        "        'error_time_vars': error_time_vars,\n",
        "        'all_time_vars': policy_time_vars + baseline_time_vars + error_time_vars\n",
        "    }\n",
        "    \n",
        "    print(f\"政策问题数量: {len(policy_questions)}\")\n",
        "    print(f\"基线问题数量: {len(baseline_questions)}\")\n",
        "    print(f\"错误阈值问题数量: {len(error_questions)}\")\n",
        "    print(f\"总时间变量数量: {len(organized_data['all_time_vars'])}\")\n",
        "    \n",
        "    return data, organized_data\n",
        "\n",
        "# Apply data preparation\n",
        "# data, organized_data = prepare_data(df)\n",
        "print(\"数据预处理函数已定义，请加载数据后运行\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Order Validation Function\n",
        "def validate_participant_orders(data):\n",
        "    \"\"\"\n",
        "    Validate that each participant's question answering order is correct\n",
        "    \"\"\"\n",
        "    print(\"验证参与者答题顺序...\")\n",
        "    \n",
        "    # Define expected orders\n",
        "    expected_orders = ['PBT', 'PTB', 'TBP', 'BTP']\n",
        "    \n",
        "    # Check if order variable exists and has valid values\n",
        "    if 'order' not in data.columns:\n",
        "        print(\"错误: 未找到 'order' 变量\")\n",
        "        return False\n",
        "    \n",
        "    # Check order distribution\n",
        "    order_counts = data['order'].value_counts()\n",
        "    print(\"排列顺序分布:\")\n",
        "    for order in expected_orders:\n",
        "        count = order_counts.get(order, 0)\n",
        "        print(f\"  {order}: {count} 参与者\")\n",
        "    \n",
        "    # Check for invalid orders\n",
        "    invalid_orders = data[~data['order'].isin(expected_orders)]['order'].unique()\n",
        "    if len(invalid_orders) > 0:\n",
        "        print(f\"警告: 发现无效的排列顺序: {invalid_orders}\")\n",
        "        return False\n",
        "    \n",
        "    print(\"所有参与者的排列顺序都有效\")\n",
        "    return True\n",
        "\n",
        "# Function to extract question sequence for each participant\n",
        "def get_question_sequence(participant_data, order):\n",
        "    \"\"\"\n",
        "    Get the question sequence for a participant based on their order\n",
        "    \"\"\"\n",
        "    sequences = {\n",
        "        'PBT': ['Policy', 'Baseline', 'Error'],\n",
        "        'PTB': ['Policy', 'Error', 'Baseline'], \n",
        "        'TBP': ['Error', 'Baseline', 'Policy'],\n",
        "        'BTP': ['Baseline', 'Error', 'Policy']\n",
        "    }\n",
        "    return sequences.get(order, [])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Response Time Analysis Functions\n",
        "def calculate_response_times(data, organized_data):\n",
        "    \"\"\"\n",
        "    Calculate response times and analyze patterns\n",
        "    \"\"\"\n",
        "    print(\"计算响应时间...\")\n",
        "    \n",
        "    # Create response time dataframe\n",
        "    response_data = []\n",
        "    \n",
        "    for idx, row in data.iterrows():\n",
        "        participant_id = idx\n",
        "        order = row['order']\n",
        "        \n",
        "        # Policy questions (single timer for all 13 questions)\n",
        "        if pd.notna(row['pref_Page Submit']):\n",
        "            response_data.append({\n",
        "                'participant_id': participant_id,\n",
        "                'question_type': 'Policy',\n",
        "                'question_sequence': 1 if order in ['PBT', 'PTB'] else 3,\n",
        "                'response_time': row['pref_Page Submit'],\n",
        "                'first_click': row['pref_First Click'],\n",
        "                'last_click': row['pref_Last Click'],\n",
        "                'click_count': row['pref_Click Count'],\n",
        "                'order': order\n",
        "            })\n",
        "        \n",
        "        # Baseline questions (7 themes, each with individual timers)\n",
        "        baseline_themes = ['disease', 'armed', 'conv', 'welfare', 'immi', 'vote', 'air', 'firearm', 'auto']\n",
        "        for i, theme in enumerate(baseline_themes):\n",
        "            page_submit_col = f'base_{theme}_Page Submit'\n",
        "            if pd.notna(row[page_submit_col]):\n",
        "                response_data.append({\n",
        "                    'participant_id': participant_id,\n",
        "                    'question_type': 'Baseline',\n",
        "                    'question_sequence': 1 if order == 'BTP' else (2 if order in ['PBT', 'TBP'] else 3),\n",
        "                    'theme': theme,\n",
        "                    'theme_order': i + 1,\n",
        "                    'response_time': row[page_submit_col],\n",
        "                    'first_click': row[f'base_{theme}_First Click'],\n",
        "                    'last_click': row[f'base_{theme}_Last Click'],\n",
        "                    'click_count': row[f'base_{theme}_Click Count'],\n",
        "                    'order': order\n",
        "                })\n",
        "        \n",
        "        # Error threshold questions (7 themes, each with individual timers)\n",
        "        error_themes = ['disease', 'armed', 'conv', 'welfare', 'immi', 'vote', 'air', 'auto']\n",
        "        for i, theme in enumerate(error_themes):\n",
        "            page_submit_col = f'err_{theme}_Page Submit'\n",
        "            if pd.notna(row[page_submit_col]):\n",
        "                response_data.append({\n",
        "                    'participant_id': participant_id,\n",
        "                    'question_type': 'Error',\n",
        "                    'question_sequence': 1 if order == 'TBP' else (2 if order in ['PTB', 'BTP'] else 3),\n",
        "                    'theme': theme,\n",
        "                    'theme_order': i + 1,\n",
        "                    'response_time': row[page_submit_col],\n",
        "                    'first_click': row[f'err_{theme}_First Click'],\n",
        "                    'last_click': row[f'err_{theme}_Last Click'],\n",
        "                    'click_count': row[f'err_{theme}_Click Count'],\n",
        "                    'order': order\n",
        "                })\n",
        "    \n",
        "    response_df = pd.DataFrame(response_data)\n",
        "    print(f\"生成了 {len(response_df)} 条响应时间记录\")\n",
        "    \n",
        "    return response_df\n",
        "\n",
        "def analyze_response_patterns(response_df):\n",
        "    \"\"\"\n",
        "    Analyze response time patterns and identify potential issues\n",
        "    \"\"\"\n",
        "    print(\"分析响应模式...\")\n",
        "    \n",
        "    # Calculate summary statistics by question type\n",
        "    summary_stats = response_df.groupby('question_type')['response_time'].agg([\n",
        "        'count', 'mean', 'median', 'std', 'min', 'max'\n",
        "    ]).round(2)\n",
        "    \n",
        "    print(\"响应时间摘要统计:\")\n",
        "    print(summary_stats)\n",
        "    \n",
        "    # Calculate response time by sequence position\n",
        "    sequence_stats = response_df.groupby(['question_type', 'question_sequence'])['response_time'].agg([\n",
        "        'count', 'mean', 'std'\n",
        "    ]).round(2)\n",
        "    \n",
        "    print(\"\\n按序列位置的响应时间:\")\n",
        "    print(sequence_stats)\n",
        "    \n",
        "    return summary_stats, sequence_stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Attention Check Functions\n",
        "def perform_attention_checks(response_df):\n",
        "    \"\"\"\n",
        "    Perform comprehensive attention checks based on response times and patterns\n",
        "    \"\"\"\n",
        "    print(\"执行注意力检查...\")\n",
        "    \n",
        "    attention_issues = []\n",
        "    \n",
        "    # 1. Check for extremely fast responses (potential random clicking)\n",
        "    for question_type in ['Policy', 'Baseline', 'Error']:\n",
        "        subset = response_df[response_df['question_type'] == question_type]\n",
        "        if len(subset) > 0:\n",
        "            # Define thresholds for \"too fast\" responses (in seconds)\n",
        "            if question_type == 'Policy':\n",
        "                min_reasonable_time = 30  # Policy questions should take at least 30 seconds\n",
        "            else:\n",
        "                min_reasonable_time = 5   # Individual baseline/error questions should take at least 5 seconds\n",
        "            \n",
        "            too_fast = subset[subset['response_time'] < min_reasonable_time]\n",
        "            if len(too_fast) > 0:\n",
        "                for _, row in too_fast.iterrows():\n",
        "                    attention_issues.append({\n",
        "                        'participant_id': row['participant_id'],\n",
        "                        'issue_type': 'Too Fast Response',\n",
        "                        'question_type': question_type,\n",
        "                        'response_time': row['response_time'],\n",
        "                        'threshold': min_reasonable_time,\n",
        "                        'severity': 'High' if row['response_time'] < min_reasonable_time/2 else 'Medium'\n",
        "                    })\n",
        "    \n",
        "    # 2. Check for extremely slow responses (potential inattention)\n",
        "    for question_type in ['Policy', 'Baseline', 'Error']:\n",
        "        subset = response_df[response_df['question_type'] == question_type]\n",
        "        if len(subset) > 0:\n",
        "            # Define thresholds for \"too slow\" responses\n",
        "            if question_type == 'Policy':\n",
        "                max_reasonable_time = 1800  # Policy questions shouldn't take more than 30 minutes\n",
        "            else:\n",
        "                max_reasonable_time = 300   # Individual questions shouldn't take more than 5 minutes\n",
        "            \n",
        "            too_slow = subset[subset['response_time'] > max_reasonable_time]\n",
        "            if len(too_slow) > 0:\n",
        "                for _, row in too_slow.iterrows():\n",
        "                    attention_issues.append({\n",
        "                        'participant_id': row['participant_id'],\n",
        "                        'issue_type': 'Too Slow Response',\n",
        "                        'question_type': question_type,\n",
        "                        'response_time': row['response_time'],\n",
        "                        'threshold': max_reasonable_time,\n",
        "                        'severity': 'High' if row['response_time'] > max_reasonable_time*2 else 'Medium'\n",
        "                    })\n",
        "    \n",
        "    # 3. Check for inconsistent response patterns within participants\n",
        "    participant_stats = response_df.groupby('participant_id').agg({\n",
        "        'response_time': ['mean', 'std', 'count'],\n",
        "        'click_count': 'mean'\n",
        "    }).round(2)\n",
        "    \n",
        "    # Flatten column names\n",
        "    participant_stats.columns = ['_'.join(col).strip() for col in participant_stats.columns]\n",
        "    \n",
        "    # Identify participants with high variability in response times\n",
        "    high_variability = participant_stats[\n",
        "        (participant_stats['response_time_std'] > participant_stats['response_time_mean'] * 0.5) &\n",
        "        (participant_stats['response_time_count'] >= 5)  # Only check participants with enough data\n",
        "    ]\n",
        "    \n",
        "    for participant_id in high_variability.index:\n",
        "        attention_issues.append({\n",
        "            'participant_id': participant_id,\n",
        "            'issue_type': 'High Response Time Variability',\n",
        "            'question_type': 'All',\n",
        "            'response_time': participant_stats.loc[participant_id, 'response_time_mean'],\n",
        "            'variability': participant_stats.loc[participant_id, 'response_time_std'],\n",
        "            'severity': 'Medium'\n",
        "        })\n",
        "    \n",
        "    # 4. Check for very low click counts (potential random clicking)\n",
        "    low_clicks = response_df[response_df['click_count'] < 2]\n",
        "    for _, row in low_clicks.iterrows():\n",
        "        attention_issues.append({\n",
        "            'participant_id': row['participant_id'],\n",
        "            'issue_type': 'Very Low Click Count',\n",
        "            'question_type': row['question_type'],\n",
        "            'response_time': row['response_time'],\n",
        "            'click_count': row['click_count'],\n",
        "            'severity': 'High' if row['click_count'] == 0 else 'Medium'\n",
        "        })\n",
        "    \n",
        "    # 5. Check for missing responses\n",
        "    missing_responses = response_df[response_df['response_time'].isna()]\n",
        "    for _, row in missing_responses.iterrows():\n",
        "        attention_issues.append({\n",
        "            'participant_id': row['participant_id'],\n",
        "            'issue_type': 'Missing Response',\n",
        "            'question_type': row['question_type'],\n",
        "            'response_time': None,\n",
        "            'severity': 'High'\n",
        "        })\n",
        "    \n",
        "    return pd.DataFrame(attention_issues)\n",
        "\n",
        "def identify_problematic_participants(attention_issues_df):\n",
        "    \"\"\"\n",
        "    Identify and categorize problematic participants\n",
        "    \"\"\"\n",
        "    print(\"识别有问题的参与者...\")\n",
        "    \n",
        "    if len(attention_issues_df) == 0:\n",
        "        print(\"未发现注意力问题\")\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    # Count issues per participant\n",
        "    issue_counts = attention_issues_df.groupby('participant_id').agg({\n",
        "        'issue_type': 'count',\n",
        "        'severity': lambda x: (x == 'High').sum()\n",
        "    }).rename(columns={'issue_type': 'total_issues', 'severity': 'high_severity_issues'})\n",
        "    \n",
        "    # Categorize participants\n",
        "    issue_counts['category'] = 'Normal'\n",
        "    issue_counts.loc[issue_counts['total_issues'] >= 3, 'category'] = 'Moderate Issues'\n",
        "    issue_counts.loc[issue_counts['high_severity_issues'] >= 2, 'category'] = 'High Issues'\n",
        "    issue_counts.loc[issue_counts['total_issues'] >= 5, 'category'] = 'Severe Issues'\n",
        "    \n",
        "    # Get detailed issue information for problematic participants\n",
        "    problematic = issue_counts[issue_counts['category'] != 'Normal']\n",
        "    \n",
        "    print(f\"发现 {len(problematic)} 个有问题的参与者:\")\n",
        "    print(problematic['category'].value_counts())\n",
        "    \n",
        "    return issue_counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Visualization Functions\n",
        "def create_comprehensive_visualizations(response_df, attention_issues_df, issue_counts):\n",
        "    \"\"\"\n",
        "    Create comprehensive visualizations of participant activities\n",
        "    \"\"\"\n",
        "    print(\"创建可视化图表...\")\n",
        "    \n",
        "    # Set up the plotting area\n",
        "    fig = plt.figure(figsize=(20, 24))\n",
        "    \n",
        "    # 1. Response Time Distribution by Question Type\n",
        "    plt.subplot(4, 3, 1)\n",
        "    for question_type in response_df['question_type'].unique():\n",
        "        subset = response_df[response_df['question_type'] == question_type]\n",
        "        plt.hist(subset['response_time'], alpha=0.7, label=question_type, bins=30)\n",
        "    plt.xlabel('Response Time (seconds)')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Response Time Distribution by Question Type')\n",
        "    plt.legend()\n",
        "    plt.yscale('log')\n",
        "    \n",
        "    # 2. Response Time by Sequence Position\n",
        "    plt.subplot(4, 3, 2)\n",
        "    sequence_data = response_df.groupby(['question_type', 'question_sequence'])['response_time'].mean().unstack()\n",
        "    sequence_data.plot(kind='bar', ax=plt.gca())\n",
        "    plt.xlabel('Question Type')\n",
        "    plt.ylabel('Average Response Time (seconds)')\n",
        "    plt.title('Average Response Time by Sequence Position')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.legend(title='Sequence Position')\n",
        "    \n",
        "    # 3. Click Count Distribution\n",
        "    plt.subplot(4, 3, 3)\n",
        "    plt.hist(response_df['click_count'], bins=20, alpha=0.7, color='skyblue')\n",
        "    plt.xlabel('Click Count')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Click Count Distribution')\n",
        "    \n",
        "    # 4. Response Time vs Click Count Scatter\n",
        "    plt.subplot(4, 3, 4)\n",
        "    for question_type in response_df['question_type'].unique():\n",
        "        subset = response_df[response_df['question_type'] == question_type]\n",
        "        plt.scatter(subset['click_count'], subset['response_time'], \n",
        "                   alpha=0.6, label=question_type, s=20)\n",
        "    plt.xlabel('Click Count')\n",
        "    plt.ylabel('Response Time (seconds)')\n",
        "    plt.title('Response Time vs Click Count')\n",
        "    plt.legend()\n",
        "    plt.yscale('log')\n",
        "    \n",
        "    # 5. Individual Participant Response Patterns\n",
        "    plt.subplot(4, 3, 5)\n",
        "    participant_sample = response_df['participant_id'].unique()[:20]  # Show first 20 participants\n",
        "    for pid in participant_sample:\n",
        "        participant_data = response_df[response_df['participant_id'] == pid]\n",
        "        plt.plot(participant_data['response_time'], alpha=0.7, linewidth=1)\n",
        "    plt.xlabel('Question Index')\n",
        "    plt.ylabel('Response Time (seconds)')\n",
        "    plt.title('Individual Participant Response Patterns (Sample)')\n",
        "    plt.yscale('log')\n",
        "    \n",
        "    # 6. Attention Issues by Type\n",
        "    plt.subplot(4, 3, 6)\n",
        "    if len(attention_issues_df) > 0:\n",
        "        issue_type_counts = attention_issues_df['issue_type'].value_counts()\n",
        "        plt.pie(issue_type_counts.values, labels=issue_type_counts.index, autopct='%1.1f%%')\n",
        "        plt.title('Distribution of Attention Issues')\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, 'No attention issues found', ha='center', va='center', transform=plt.gca().transAxes)\n",
        "        plt.title('Distribution of Attention Issues')\n",
        "    \n",
        "    # 7. Problematic Participants Overview\n",
        "    plt.subplot(4, 3, 7)\n",
        "    if len(issue_counts) > 0:\n",
        "        category_counts = issue_counts['category'].value_counts()\n",
        "        colors = ['green' if cat == 'Normal' else 'orange' if 'Moderate' in cat else 'red' for cat in category_counts.index]\n",
        "        plt.bar(category_counts.index, category_counts.values, color=colors)\n",
        "        plt.xlabel('Participant Category')\n",
        "        plt.ylabel('Number of Participants')\n",
        "        plt.title('Participant Categories by Issue Severity')\n",
        "        plt.xticks(rotation=45)\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, 'No problematic participants', ha='center', va='center', transform=plt.gca().transAxes)\n",
        "        plt.title('Participant Categories by Issue Severity')\n",
        "    \n",
        "    # 8. Response Time Trends by Order\n",
        "    plt.subplot(4, 3, 8)\n",
        "    order_trends = response_df.groupby(['order', 'question_type'])['response_time'].mean().unstack()\n",
        "    order_trends.plot(kind='bar', ax=plt.gca())\n",
        "    plt.xlabel('Order')\n",
        "    plt.ylabel('Average Response Time (seconds)')\n",
        "    plt.title('Response Time by Order and Question Type')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.legend(title='Question Type')\n",
        "    \n",
        "    # 9. Baseline vs Error Question Comparison\n",
        "    plt.subplot(4, 3, 9)\n",
        "    baseline_data = response_df[response_df['question_type'] == 'Baseline']\n",
        "    error_data = response_df[response_df['question_type'] == 'Error']\n",
        "    \n",
        "    if len(baseline_data) > 0 and len(error_data) > 0:\n",
        "        plt.boxplot([baseline_data['response_time'], error_data['response_time']], \n",
        "                   labels=['Baseline', 'Error'])\n",
        "        plt.ylabel('Response Time (seconds)')\n",
        "        plt.title('Baseline vs Error Questions Response Time')\n",
        "        plt.yscale('log')\n",
        "    \n",
        "    # 10. Click Count by Question Type\n",
        "    plt.subplot(4, 3, 10)\n",
        "    click_data = response_df.groupby('question_type')['click_count'].apply(list)\n",
        "    plt.boxplot(click_data.values, labels=click_data.index)\n",
        "    plt.xlabel('Question Type')\n",
        "    plt.ylabel('Click Count')\n",
        "    plt.title('Click Count Distribution by Question Type')\n",
        "    \n",
        "    # 11. Response Time Heatmap by Participant and Question Type\n",
        "    plt.subplot(4, 3, 11)\n",
        "    if len(response_df) > 0:\n",
        "        # Create a pivot table for heatmap\n",
        "        heatmap_data = response_df.pivot_table(\n",
        "            values='response_time', \n",
        "            index='participant_id', \n",
        "            columns='question_type', \n",
        "            aggfunc='mean'\n",
        "        )\n",
        "        # Show only first 30 participants for readability\n",
        "        heatmap_subset = heatmap_data.head(30)\n",
        "        sns.heatmap(heatmap_subset, cmap='YlOrRd', cbar_kws={'label': 'Response Time (seconds)'})\n",
        "        plt.title('Response Time Heatmap (First 30 Participants)')\n",
        "        plt.xlabel('Question Type')\n",
        "        plt.ylabel('Participant ID')\n",
        "    \n",
        "    # 12. Issue Severity Distribution\n",
        "    plt.subplot(4, 3, 12)\n",
        "    if len(attention_issues_df) > 0:\n",
        "        severity_counts = attention_issues_df['severity'].value_counts()\n",
        "        colors = {'High': 'red', 'Medium': 'orange', 'Low': 'yellow'}\n",
        "        severity_colors = [colors.get(sev, 'gray') for sev in severity_counts.index]\n",
        "        plt.bar(severity_counts.index, severity_counts.values, color=severity_colors)\n",
        "        plt.xlabel('Issue Severity')\n",
        "        plt.ylabel('Number of Issues')\n",
        "        plt.title('Distribution of Issue Severity')\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, 'No issues found', ha='center', va='center', transform=plt.gca().transAxes)\n",
        "        plt.title('Distribution of Issue Severity')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return fig\n",
        "\n",
        "def create_individual_participant_analysis(response_df, problematic_participants):\n",
        "    \"\"\"\n",
        "    Create detailed analysis for individual problematic participants\n",
        "    \"\"\"\n",
        "    if len(problematic_participants) == 0:\n",
        "        print(\"没有发现有问题参与者，跳过个体分析\")\n",
        "        return\n",
        "    \n",
        "    print(\"创建个体参与者详细分析...\")\n",
        "    \n",
        "    # Get top 10 most problematic participants\n",
        "    top_problematic = problematic_participants.nlargest(10, 'total_issues')\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 5, figsize=(25, 10))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for i, (participant_id, row) in enumerate(top_problematic.iterrows()):\n",
        "        if i >= 10:\n",
        "            break\n",
        "            \n",
        "        participant_data = response_df[response_df['participant_id'] == participant_id]\n",
        "        \n",
        "        # Plot response times by question type\n",
        "        for j, question_type in enumerate(['Policy', 'Baseline', 'Error']):\n",
        "            subset = participant_data[participant_data['question_type'] == question_type]\n",
        "            if len(subset) > 0:\n",
        "                axes[i].scatter(subset['response_time'], [j] * len(subset), \n",
        "                              label=question_type, alpha=0.7, s=50)\n",
        "        \n",
        "        axes[i].set_xlabel('Response Time (seconds)')\n",
        "        axes[i].set_ylabel('Question Type')\n",
        "        axes[i].set_title(f'Participant {participant_id}\\nIssues: {row[\"total_issues\"]}, High: {row[\"high_severity_issues\"]}')\n",
        "        axes[i].set_yscale('log')\n",
        "        axes[i].set_yticks([0, 1, 2])\n",
        "        axes[i].set_yticklabels(['Policy', 'Baseline', 'Error'])\n",
        "        axes[i].legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Main Analysis Pipeline\n",
        "def run_complete_analysis(df):\n",
        "    \"\"\"\n",
        "    Run the complete attention check analysis pipeline\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"开始完整的注意力检查分析\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Step 1: Data Preparation\n",
        "    print(\"\\n1. 数据预处理...\")\n",
        "    data, organized_data = prepare_data(df)\n",
        "    \n",
        "    # Step 2: Order Validation\n",
        "    print(\"\\n2. 验证答题顺序...\")\n",
        "    order_valid = validate_participant_orders(data)\n",
        "    if not order_valid:\n",
        "        print(\"警告: 发现答题顺序问题，请检查数据\")\n",
        "    \n",
        "    # Step 3: Calculate Response Times\n",
        "    print(\"\\n3. 计算响应时间...\")\n",
        "    response_df = calculate_response_times(data, organized_data)\n",
        "    \n",
        "    # Step 4: Analyze Response Patterns\n",
        "    print(\"\\n4. 分析响应模式...\")\n",
        "    summary_stats, sequence_stats = analyze_response_patterns(response_df)\n",
        "    \n",
        "    # Step 5: Perform Attention Checks\n",
        "    print(\"\\n5. 执行注意力检查...\")\n",
        "    attention_issues_df = perform_attention_checks(response_df)\n",
        "    \n",
        "    # Step 6: Identify Problematic Participants\n",
        "    print(\"\\n6. 识别有问题的参与者...\")\n",
        "    issue_counts = identify_problematic_participants(attention_issues_df)\n",
        "    \n",
        "    # Step 7: Create Visualizations\n",
        "    print(\"\\n7. 创建可视化图表...\")\n",
        "    fig = create_comprehensive_visualizations(response_df, attention_issues_df, issue_counts)\n",
        "    \n",
        "    # Step 8: Individual Participant Analysis\n",
        "    print(\"\\n8. 个体参与者分析...\")\n",
        "    create_individual_participant_analysis(response_df, issue_counts)\n",
        "    \n",
        "    # Step 9: Generate Summary Report\n",
        "    print(\"\\n9. 生成摘要报告...\")\n",
        "    generate_summary_report(response_df, attention_issues_df, issue_counts)\n",
        "    \n",
        "    return {\n",
        "        'response_df': response_df,\n",
        "        'attention_issues_df': attention_issues_df,\n",
        "        'issue_counts': issue_counts,\n",
        "        'summary_stats': summary_stats,\n",
        "        'sequence_stats': sequence_stats\n",
        "    }\n",
        "\n",
        "def generate_summary_report(response_df, attention_issues_df, issue_counts):\n",
        "    \"\"\"\n",
        "    Generate a comprehensive summary report\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"分析摘要报告\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Basic statistics\n",
        "    total_participants = response_df['participant_id'].nunique()\n",
        "    total_responses = len(response_df)\n",
        "    \n",
        "    print(f\"\\n基本统计:\")\n",
        "    print(f\"  总参与者数量: {total_participants}\")\n",
        "    print(f\"  总响应数量: {total_responses}\")\n",
        "    print(f\"  平均每个参与者响应数: {total_responses/total_participants:.1f}\")\n",
        "    \n",
        "    # Response time statistics\n",
        "    print(f\"\\n响应时间统计:\")\n",
        "    for question_type in response_df['question_type'].unique():\n",
        "        subset = response_df[response_df['question_type'] == question_type]\n",
        "        avg_time = subset['response_time'].mean()\n",
        "        median_time = subset['response_time'].median()\n",
        "        print(f\"  {question_type}: 平均 {avg_time:.1f}秒, 中位数 {median_time:.1f}秒\")\n",
        "    \n",
        "    # Attention issues summary\n",
        "    if len(attention_issues_df) > 0:\n",
        "        print(f\"\\n注意力问题统计:\")\n",
        "        print(f\"  总问题数量: {len(attention_issues_df)}\")\n",
        "        print(f\"  涉及参与者: {attention_issues_df['participant_id'].nunique()}\")\n",
        "        \n",
        "        issue_type_summary = attention_issues_df['issue_type'].value_counts()\n",
        "        print(f\"  问题类型分布:\")\n",
        "        for issue_type, count in issue_type_summary.items():\n",
        "            print(f\"    {issue_type}: {count}\")\n",
        "        \n",
        "        severity_summary = attention_issues_df['severity'].value_counts()\n",
        "        print(f\"  严重程度分布:\")\n",
        "        for severity, count in severity_summary.items():\n",
        "            print(f\"    {severity}: {count}\")\n",
        "    else:\n",
        "        print(f\"\\n注意力问题统计: 未发现注意力问题\")\n",
        "    \n",
        "    # Problematic participants summary\n",
        "    if len(issue_counts) > 0:\n",
        "        print(f\"\\n有问题参与者统计:\")\n",
        "        category_summary = issue_counts['category'].value_counts()\n",
        "        for category, count in category_summary.items():\n",
        "            print(f\"  {category}: {count} 参与者\")\n",
        "        \n",
        "        # List most problematic participants\n",
        "        most_problematic = issue_counts.nlargest(5, 'total_issues')\n",
        "        print(f\"\\n问题最多的5个参与者:\")\n",
        "        for participant_id, row in most_problematic.iterrows():\n",
        "            print(f\"  参与者 {participant_id}: {row['total_issues']} 个问题, {row['high_severity_issues']} 个高严重性问题\")\n",
        "    else:\n",
        "        print(f\"\\n有问题参与者统计: 未发现有问题参与者\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"分析完成\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "# Example usage (uncomment when you have your data loaded)\n",
        "# results = run_complete_analysis(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Additional Analysis Functions for Specific Requirements\n",
        "\n",
        "def compare_question_difficulty(response_df):\n",
        "    \"\"\"\n",
        "    Compare the same question with its mean completion speed from all samples\n",
        "    \"\"\"\n",
        "    print(\"比较问题难度...\")\n",
        "    \n",
        "    # Group by question type and calculate mean response times\n",
        "    question_means = response_df.groupby('question_type')['response_time'].mean()\n",
        "    \n",
        "    # For baseline and error questions, also compare by theme\n",
        "    baseline_data = response_df[response_df['question_type'] == 'Baseline']\n",
        "    error_data = response_df[response_df['question_type'] == 'Error']\n",
        "    \n",
        "    if len(baseline_data) > 0:\n",
        "        baseline_theme_means = baseline_data.groupby('theme')['response_time'].mean().sort_values()\n",
        "        print(\"\\n基线问题主题平均响应时间:\")\n",
        "        for theme, mean_time in baseline_theme_means.items():\n",
        "            print(f\"  {theme}: {mean_time:.1f}秒\")\n",
        "    \n",
        "    if len(error_data) > 0:\n",
        "        error_theme_means = error_data.groupby('theme')['response_time'].mean().sort_values()\n",
        "        print(\"\\n错误阈值问题主题平均响应时间:\")\n",
        "        for theme, mean_time in error_theme_means.items():\n",
        "            print(f\"  {theme}: {mean_time:.1f}秒\")\n",
        "    \n",
        "    return question_means\n",
        "\n",
        "def analyze_sequential_patterns(response_df):\n",
        "    \"\"\"\n",
        "    Compare questions in sequential order with their counterparts\n",
        "    \"\"\"\n",
        "    print(\"分析序列模式...\")\n",
        "    \n",
        "    # Analyze baseline questions as a group\n",
        "    baseline_data = response_df[response_df['question_type'] == 'Baseline']\n",
        "    if len(baseline_data) > 0:\n",
        "        # Calculate average response time by sequence position for baseline\n",
        "        baseline_sequence = baseline_data.groupby('theme_order')['response_time'].agg(['mean', 'std', 'count'])\n",
        "        print(\"\\n基线问题序列位置分析:\")\n",
        "        for pos, row in baseline_sequence.iterrows():\n",
        "            print(f\"  位置 {pos}: 平均 {row['mean']:.1f}秒 (标准差: {row['std']:.1f}, 样本数: {row['count']})\")\n",
        "    \n",
        "    # Analyze error threshold questions as a group\n",
        "    error_data = response_df[response_df['question_type'] == 'Error']\n",
        "    if len(error_data) > 0:\n",
        "        # Calculate average response time by sequence position for error\n",
        "        error_sequence = error_data.groupby('theme_order')['response_time'].agg(['mean', 'std', 'count'])\n",
        "        print(\"\\n错误阈值问题序列位置分析:\")\n",
        "        for pos, row in error_sequence.iterrows():\n",
        "            print(f\"  位置 {pos}: 平均 {row['mean']:.1f}秒 (标准差: {row['std']:.1f}, 样本数: {row['count']})\")\n",
        "    \n",
        "    return baseline_sequence if len(baseline_data) > 0 else None, error_sequence if len(error_data) > 0 else None\n",
        "\n",
        "def create_sequence_visualization(response_df):\n",
        "    \"\"\"\n",
        "    Create visualization for sequential patterns\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "    \n",
        "    # Policy questions (single timer)\n",
        "    policy_data = response_df[response_df['question_type'] == 'Policy']\n",
        "    if len(policy_data) > 0:\n",
        "        axes[0].hist(policy_data['response_time'], bins=20, alpha=0.7, color='blue')\n",
        "        axes[0].set_xlabel('Response Time (seconds)')\n",
        "        axes[0].set_ylabel('Frequency')\n",
        "        axes[0].set_title('Policy Questions Response Time')\n",
        "        axes[0].set_yscale('log')\n",
        "    \n",
        "    # Baseline questions by sequence position\n",
        "    baseline_data = response_df[response_df['question_type'] == 'Baseline']\n",
        "    if len(baseline_data) > 0:\n",
        "        baseline_sequence = baseline_data.groupby('theme_order')['response_time'].mean()\n",
        "        axes[1].plot(baseline_sequence.index, baseline_sequence.values, 'o-', color='green', linewidth=2, markersize=8)\n",
        "        axes[1].set_xlabel('Question Sequence Position')\n",
        "        axes[1].set_ylabel('Average Response Time (seconds)')\n",
        "        axes[1].set_title('Baseline Questions: Response Time by Position')\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Error threshold questions by sequence position\n",
        "    error_data = response_df[response_df['question_type'] == 'Error']\n",
        "    if len(error_data) > 0:\n",
        "        error_sequence = error_data.groupby('theme_order')['response_time'].mean()\n",
        "        axes[2].plot(error_sequence.index, error_sequence.values, 'o-', color='red', linewidth=2, markersize=8)\n",
        "        axes[2].set_xlabel('Question Sequence Position')\n",
        "        axes[2].set_ylabel('Average Response Time (seconds)')\n",
        "        axes[2].set_title('Error Threshold Questions: Response Time by Position')\n",
        "        axes[2].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Function to export results\n",
        "def export_results(results, filename_prefix=\"attention_analysis\"):\n",
        "    \"\"\"\n",
        "    Export analysis results to CSV files\n",
        "    \"\"\"\n",
        "    print(\"导出分析结果...\")\n",
        "    \n",
        "    # Export response data\n",
        "    results['response_df'].to_csv(f\"{filename_prefix}_response_data.csv\", index=False)\n",
        "    print(f\"响应数据已导出到: {filename_prefix}_response_data.csv\")\n",
        "    \n",
        "    # Export attention issues\n",
        "    if len(results['attention_issues_df']) > 0:\n",
        "        results['attention_issues_df'].to_csv(f\"{filename_prefix}_attention_issues.csv\", index=False)\n",
        "        print(f\"注意力问题已导出到: {filename_prefix}_attention_issues.csv\")\n",
        "    \n",
        "    # Export participant issue counts\n",
        "    if len(results['issue_counts']) > 0:\n",
        "        results['issue_counts'].to_csv(f\"{filename_prefix}_participant_issues.csv\", index=True)\n",
        "        print(f\"参与者问题统计已导出到: {filename_prefix}_participant_issues.csv\")\n",
        "    \n",
        "    print(\"所有结果已成功导出\")\n",
        "\n",
        "print(\"所有分析函数已定义完成！\")\n",
        "print(\"\\n使用方法:\")\n",
        "print(\"1. 加载您的数据集到变量 'df'\")\n",
        "print(\"2. 运行: results = run_complete_analysis(df)\")\n",
        "print(\"3. 可选: export_results(results) 导出结果\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}